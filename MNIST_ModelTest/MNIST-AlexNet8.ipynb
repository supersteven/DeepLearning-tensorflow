{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least two variables have the same name: Variable/Adam",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c66b68085705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#启动创建的模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1336\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[0;32m   1337\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1382\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[0;32m   1385\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m       \u001b[1;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[1;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[0;32m    811\u001b[0m                        \" when eager execution is not enabled.\")\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m     \u001b[0msaveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[1;34m(self, names_to_saveables)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \"\"\"\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m       \u001b[0mnames_to_saveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpListToDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[0msaveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mOpListToDict\u001b[1;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[0;32m    636\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[1;32m--> 638\u001b[1;33m                              name)\n\u001b[0m\u001b[0;32m    639\u001b[0m           \u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: At least two variables have the same name: Variable/Adam"
     ]
    }
   ],
   "source": [
    "# AlexNet 8 (Train-Test)\n",
    "\n",
    "#coding=utf-8\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "start = time.clock() #计算开始时间\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/MNIST_data\", one_hot=True)\n",
    "\n",
    "# 定义网络超参数\n",
    "learning_rate = 0.001\n",
    "training_iters = 2000\n",
    "batch_size = 64\n",
    "display_step = 20\n",
    "\n",
    "# 定义网络参数\n",
    "n_input = 784 # 输入的维度\n",
    "n_classes = 10 # 标签的维度\n",
    "dropout = 0.8 # Dropout 的概率\n",
    "\n",
    "# 占位符输入\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 卷积操作\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "# 最大下采样操作\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "# 归一化操作\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "# 定义整个网络 \n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    # 向量转为矩阵\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # 卷积层\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    # 下采样层\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    # 归一化层\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    # Dropout\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # 卷积\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    # 下采样\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    # 归一化\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Dropout\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # 卷积\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    # 下采样\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    # 归一化\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Dropout\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "    # 全连接层，先把特征图转为向量\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') \n",
    "    # 全连接层\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # 网络输出层\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out\n",
    "\n",
    "# 存储所有的网络参数\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([128])),\n",
    "    'bc3': tf.Variable(tf.random_normal([256])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 构建模型\n",
    "pred = alex_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# 定义损失函数和学习步骤\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 测试网络\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession(config=config) #启动创建的模型\n",
    "sess.run(tf.global_variables_initializer()) #初始化变量\n",
    "\n",
    "# 开启一个训练\n",
    "step = 1\n",
    "# Keep training until reach max iterations\n",
    "while step * batch_size < training_iters:\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    # 获取批数据\n",
    "    sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "    if step % display_step == 0:\n",
    "        # 计算精度\n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "        # 计算损失值\n",
    "        loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "        print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy = \" + \"{:.5f}\".format(acc))\n",
    "    step += 1\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# 计算测试精度\n",
    "print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n",
    "\n",
    "saver.save(sess, './model/mymodel', step*batch_size)\n",
    "\n",
    "end = time.clock() #计算程序结束时间\n",
    "print((\"running time is %g s\") % (end-start))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6c9983d3fcce>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-6c9983d3fcce>:105: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/mymodel.ckpt-200000\n",
      "Testing Accuracy: 0.957031\n",
      "running time is 3.43138 s\n"
     ]
    }
   ],
   "source": [
    "# AlexNet 8 (Test from saved model)\n",
    "\n",
    "#coding=utf-8\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "start = time.clock() #计算开始时间\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/MNIST_data\", one_hot=True)\n",
    "\n",
    "# 定义网络超参数\n",
    "learning_rate = 0.001\n",
    "training_iters = 2000\n",
    "batch_size = 64\n",
    "display_step = 20\n",
    "\n",
    "# 定义网络参数\n",
    "n_input = 784 # 输入的维度\n",
    "n_classes = 10 # 标签的维度\n",
    "dropout = 0.8 # Dropout 的概率\n",
    "\n",
    "# 占位符输入\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 卷积操作\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "# 最大下采样操作\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "# 归一化操作\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "# 定义整个网络 \n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    # 向量转为矩阵\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # 卷积层\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    # 下采样层\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    # 归一化层\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    # Dropout\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # 卷积\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    # 下采样\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    # 归一化\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Dropout\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # 卷积\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    # 下采样\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    # 归一化\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Dropout\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "    # 全连接层，先把特征图转为向量\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') \n",
    "    # 全连接层\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # 网络输出层\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out\n",
    "\n",
    "# 存储所有的网络参数\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([128])),\n",
    "    'bc3': tf.Variable(tf.random_normal([256])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 构建模型\n",
    "pred = alex_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# 定义损失函数和学习步骤\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 测试网络\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "sess = tf.InteractiveSession(config=config) #启动创建的模型\n",
    "sess.run(tf.global_variables_initializer()) #初始化变量\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#saver = tf.train.import_meta_graph('./model/mymodel.ckpt-200000.meta')\n",
    "#读取上面训练好的模型参数\n",
    "saver.restore(sess, './model/mymodel.ckpt-200000')\n",
    "print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n",
    "sess.close()\n",
    "\n",
    "end = time.clock() #计算程序结束时间\n",
    "print((\"running time is %g s\") % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-27-8ad3e672e585>:118: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8ad3e672e585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#启动创建的模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#初始化变量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1305\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1307\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1338\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "import time\n",
    "\n",
    "image_size = 784 #28*28\n",
    "class_num = 10# 0~10\n",
    "\n",
    "#共训练100000次\n",
    "total_step = 100000\n",
    "#每隔10×batch_size步显示一次结果\n",
    "display_step = 10\n",
    "#学习率\n",
    "learning_rate = 0.01\n",
    "#每次找出50张图片进行训练\n",
    "batch_size = 50\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) #MNIST数据输入\n",
    "\n",
    "#image的placehol der\n",
    "x = tf.placeholder(tf.float32, [None,image_size])\n",
    "#label的placeholder\n",
    "y = tf.placeholder(tf.float32, [None,class_num])\n",
    "#dropout的placeholder\n",
    "dropoutP = tf.placeholder(tf.float32)\n",
    "\n",
    "#卷积层定义\n",
    "def conv2d(image_input,w,b,name):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(image_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "#下采样层定义\n",
    "def pooling(featuremaps,kernel_size,name):\n",
    "    return tf.nn.max_pool(featuremaps, [1,kernel_size,kernel_size,1], [1,kernel_size,kernel_size,1], padding='SAME')\n",
    "\n",
    "#归一化操作\n",
    "def normlize(featuremaps,l_size,name):\n",
    "    return tf.nn.lrn(featuremaps, 4, bias=1, alpha=0.0001, beta=0.75)\n",
    "\n",
    "#初始化参数\n",
    "weights = {\n",
    "           #[3,3,1,64]分别代表3*3*1的kernel，输入层有1个feature maps，输出层共64个feature maps\n",
    "           'wc1' : tf.Variable(tf.random_normal([3,3,1,64])),\n",
    "           #[3,3,64,128]分别代表3*3*64的kernel，输入层有64个feature maps，输出层有128个feature maps\n",
    "           'wc2' : tf.Variable(tf.random_normal([3,3,64,128])),\n",
    "           'wc3' : tf.Variable(tf.random_normal([3,3,128,256])),\n",
    "           'wc4' : tf.Variable(tf.random_normal([2,2,256,512])),\n",
    "           #全连接层的参数个数设定的原则好像是经过卷积层运算以后feature map的大小并没有发生改变\n",
    "           #发生改变的原因都是pooling层28/2/2/2/2 = 2(7/2可能是为4）\n",
    "           'wd1' : tf.Variable(tf.random_normal([2*2*512,1024])),\n",
    "           'wd2' : tf.Variable(tf.random_normal([1024,1024])),\n",
    "           'out' : tf.Variable(tf.random_normal([1024,10]))\n",
    "           }\n",
    "#初始化偏置项\n",
    "biases = {\n",
    "          'bc1' : tf.Variable(tf.random_normal([64])),\n",
    "          'bc2' : tf.Variable(tf.random_normal([128])),\n",
    "          'bc3' : tf.Variable(tf.random_normal([256])),\n",
    "          'bc4' : tf.Variable(tf.random_normal([512])),\n",
    "          'bd1' : tf.Variable(tf.random_normal([1024])),\n",
    "          'bd2' : tf.Variable(tf.random_normal([1024])),\n",
    "          'out' : tf.Variable(tf.random_normal([10]))\n",
    "          }\n",
    "#构建网络\n",
    "def constructNet(images,weights,biases,_dropout):\n",
    "    #首先把图片转为28*28*1的tensor\n",
    "    images = tf.reshape(images,[-1,28,28,1])\n",
    "\n",
    "    #第一个卷积层conv1\n",
    "    conv1 = conv2d(images, weights['wc1'], biases['bc1'], 'conv1')\n",
    "#    print 'conv1: ',conv1.get_shape()\n",
    "    #卷积层conv1对应下采样层\n",
    "    pool1 = pooling(conv1, 2, 'pool1')\n",
    "#    print 'pool1: ',pool1.get_shape()\n",
    "    #归一化\n",
    "    norm1 = normlize(pool1, l_size=4, name='norm1')\n",
    "    dropout1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    #第二个卷积层\n",
    "    conv2 = conv2d(dropout1,weights['wc2'],biases['bc2'],'conv2')\n",
    "#    print 'conv2: ',conv2.get_shape()\n",
    "    pool2 = pooling(conv2, 2, 'pool2')\n",
    "#    print 'pool2: ',pool2.get_shape()\n",
    "    norm2 = normlize(pool2, 4, 'norm2')\n",
    "    dropout2 = tf.nn.dropout(norm2,_dropout)\n",
    "\n",
    "    #第三个卷积层\n",
    "    conv3 = conv2d(dropout2, weights['wc3'], biases['bc3'], 'conv3')\n",
    "#    print 'conv3: ',conv3.get_shape()\n",
    "    pool3 = pooling(conv3, 2, 'pool3')\n",
    "#    print 'pool3: ',pool3.get_shape()\n",
    "    norm3 = normlize(pool3, 4, 'norm3')\n",
    "    dropout3 = tf.nn.dropout(norm3,_dropout)\n",
    "\n",
    "    #第四个卷积层\n",
    "    conv4 = conv2d(dropout3,weights['wc4'],biases['bc4'],'conv4')\n",
    "#    print 'conv4: ',conv4.get_shape()\n",
    "    pool4 = pooling(conv4, 2, 'pool4')\n",
    "#    print 'pool4: ',pool4.get_shape()\n",
    "    norm4 = normlize(pool4, 4, 'norm4')\n",
    "#    print 'norm4: ',norm4.get_shape()\n",
    "    #全链接层1\n",
    "    dense1 = tf.reshape(norm4, [-1,weights['wd1'].get_shape().as_list()[0]])\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1,weights['wd1']) + biases['bd1'],'fc1')\n",
    "    #全链接层2\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1,weights['wd2']) + biases['bd2'],'fc2')\n",
    "    #输出层，最后输出层不需要激活函数relu操作\n",
    "    out = tf.matmul(dense2,weights['out']) + biases['out']\n",
    "    return out\n",
    "\n",
    "pred = constructNet(x, weights, biases, dropoutP)\n",
    "#计算loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "#定义操作，用以最小化loss（Adam是一种梯度下降算法）\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#tf.arg_max(pred,1)是按行取最大值的下标\n",
    "#tf.arg_max(y,1)是按列取最大值的下标\n",
    "correct_pred = tf.equal(tf.arg_max(pred,1), tf.arg_max(y,1))\n",
    "#先将correct_pred中数据格式转换为float32类型\n",
    "#求correct_pred中的平均值，因为correct_pred中除了0就是1，因此求平均值即为1的所占比例，即正确率\n",
    "correct_rate = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "sess = tf.InteractiveSession(config=config) #启动创建的模型\n",
    "sess.run(tf.global_variables_initializer()) #初始化变量\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "step = 1\n",
    "while step*batch_size < total_step:\n",
    "    batchx,batchy = mnist.train.next_batch(batch_size)\n",
    "    sess.run(optimizer,feed_dict={x:batchx,y:batchy,dropoutP:0.75})\n",
    "    if(step % display_step == 0):\n",
    "        accracy = sess.run(correct_rate,feed_dict={x:batchx,y:batchy,dropoutP:1.})\n",
    "        cost = sess.run(loss,feed_dict={x:batchx,y:batchy,dropoutP:1.})\n",
    "        print(\"step: %d, cost: %g, train_accuracy: %g\" %(step*batch_size, cost, accracy))\n",
    "#            print 'Step: ' + str(step*batch_size) + ' cost: ' + str(cost) + ' accracy: ' + str(accracy)\n",
    "        #保存当前网络的参数，以便测试时读取训练结果\n",
    "        saver.save(sess, '/model/steven.netmodel',step)\n",
    "\n",
    "    step += 1\n",
    "#    print 'train Finished'\n",
    "\n",
    "#读取上面训练好的模型参数\n",
    "saver.restore(sess, '/model/steven.netmodel-1990')\n",
    "#    print 'Testing accary: ',sess.run(correct_rate,feed_dict={x:mnist.test.images[:100],y:mnist.test.labels[:100],dropoutP:1.})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
